{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here it is checked which slices contain information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZP1_z = '0_00097_ZP1_Z_cls'\n",
    "ZP1_p = '0_00099_ZP1_P_3_cls'\n",
    "\n",
    "ZP2_z = '0_00093_ZP2_Z_cls'\n",
    "ZP2_p = '0_00095_ZP2_P_6_cls'\n",
    "\n",
    "ZP3_z = '0_00089_ZP3_Z_cls'\n",
    "ZP3_p = '0_00090_ZP3_P_9_cls'\n",
    "\n",
    "ZP4_z = '0_00109_ZP4_Z_cls'\n",
    "ZP4_p = '0_00110_ZP4_P_2_cls'\n",
    "\n",
    "ZP5_z = '0_00105_ZP5_Z_cls'\n",
    "ZP5_p = '0_00106_ZP5_P_5_cls'\n",
    "\n",
    "ZP6_z = '0_00101_ZP6_Z_cls'\n",
    "ZP6_p = '0_00103_ZP6_P_8_cls'\n",
    "\n",
    "ZP7_z = '0_00122_ZP7_Z_cls'\n",
    "ZP7_p = '0_00123_ZP7_P_cls'\n",
    "\n",
    "ZP8_z = '0_00117_ZP8_Z_cls'\n",
    "ZP8_p = '0_00118_ZP8_P_4_cls'\n",
    "\n",
    "ZP9_z = '0_00113_ZP9_Z_cls'\n",
    "ZP9_p = '0_00114_ZP9_P_7_cls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_buildjob_hdf5 = '/home/jan/Documents/Klein_Datentransfer/HDF/BJ_Superlativ_QualiPro.h5'\n",
    "number = 9\n",
    "builjob_name_z = ZP9_z\n",
    "builjob_name_p = ZP9_p\n",
    "new_file_path = '/home/jan/Documents/Trainingsdaten/ZPs/ZP{}/ZP_{}_full_part.h5'.format(number, number)\n",
    "name_in_h5 = 'ZP{}_combined'.format(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_z_param = []\n",
    "with h5py.File(path_buildjob_hdf5,'a') as h5:\n",
    "    key_list = h5[builjob_name_z].keys()\n",
    "    \n",
    "    for key in key_list:\n",
    "       #shape suchen \n",
    "        shape = h5[builjob_name_z][key]['Area'].shape[0]\n",
    "        if shape > 0: \n",
    "            list_z_param.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_p_param = []\n",
    "with h5py.File(path_buildjob_hdf5,'a') as h5:\n",
    "    key_list = h5[builjob_name_p].keys()\n",
    "    \n",
    "    for key in key_list:\n",
    "       #shape suchen \n",
    "        shape = h5[builjob_name_p][key]['Area'].shape[0]\n",
    "        if shape > 0: \n",
    "            list_p_param.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_hdf = h5py.File(new_file_path, \"w\")\n",
    "Training_hdf.close()\n",
    "\n",
    "with h5py.File(new_file_path,'a') as h5:\n",
    "    h5.create_group(name_in_h5)\n",
    "    for slice_num in list_z_param:\n",
    "        #the following block transforms the uneven layer numbers in normal numbers\n",
    "        slice_num_int = int(re.search(r'\\d+', slice_num).group())\n",
    "        slice_num_normal = math.trunc(slice_num_int/2)\n",
    "        slice_name_normal = 'Slice' + str(\"{:05d}\".format(slice_num_normal))  \n",
    "        \n",
    "        h5[name_in_h5].create_group(slice_name_normal)\n",
    "        with h5py.File(path_buildjob_hdf5,'a') as h5_2:\n",
    "            h5[name_in_h5][slice_name_normal].create_dataset('Area',data = h5_2[builjob_name_z][slice_num]['Area'])\n",
    "            h5[name_in_h5][slice_name_normal].create_dataset('Intensity',data = h5_2[builjob_name_z][slice_num]['Intensity'])\n",
    "            h5[name_in_h5][slice_name_normal].create_dataset('X-Axis',data = h5_2[builjob_name_z][slice_num]['X-Axis'])\n",
    "            h5[name_in_h5][slice_name_normal].create_dataset('Y-Axis',data = h5_2[builjob_name_z][slice_num]['Y-Axis'])\n",
    "            \n",
    "    for slice_num in list_p_param:\n",
    "        #the following block transforms the uneven layer numbers in normal numbers\n",
    "        slice_num_int_p = int(re.search(r'\\d+', slice_num).group())\n",
    "        slice_num_normal_p = math.trunc(slice_num_int_p/2)\n",
    "        slice_name_normal_p = 'Slice' + str(\"{:05d}\".format(slice_num_normal_p))\n",
    "        \n",
    "        h5[name_in_h5].create_group(slice_name_normal_p)\n",
    "        with h5py.File(path_buildjob_hdf5,'a') as h5_2:\n",
    "            h5[name_in_h5][slice_name_normal_p].create_dataset('Area',data = h5_2[builjob_name_p][slice_num]['Area'])\n",
    "            h5[name_in_h5][slice_name_normal_p].create_dataset('Intensity',data = h5_2[builjob_name_p][slice_num]['Intensity'])\n",
    "            h5[name_in_h5][slice_name_normal_p].create_dataset('X-Axis',data = h5_2[builjob_name_p][slice_num]['X-Axis'])\n",
    "            h5[name_in_h5][slice_name_normal_p].create_dataset('Y-Axis',data = h5_2[builjob_name_p][slice_num]['Y-Axis'])        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataPrep]",
   "language": "python",
   "name": "conda-env-DataPrep-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
